---
title: "Conducting mixed effect/trajectory analysis using DataSHIELD"
author: Tim Cadman
output:
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
---
```{r settings, echo = FALSE}
options(datashield.progress = FALSE)
options(datashield.progress.clear = TRUE)
knitr::opts_chunk$set(message = FALSE)
knitr::opts_chunk$set(results = FALSE)
knitr::opts_chunk$set(warning = FALSE)
options(knitr.duplicate.label = "allow")
```
# Introduction
This tutorial uses simulated data to demonstrate how to conduct mixed effect
analyses with two-stage meta-analysis using DataSHIELD.

This tutorial draws on the following papers/tutorials:

1. Hughes, R., Tilling, K. & Lawlor, D. Combining longitudinal data from 
different cohorts to examine the life-course trajectory. Preprint available on 
medrxiv: https://doi.org/10.1101/2020.11.24.20237669 

2. Tilling K, Macdonald-Wallis C, Lawlor DA, Hughes RA, Howe LD. Modelling 
childhood growth using fractional polynomials and linear splines. Ann Nutr 
Metab. 2014;65(2-3):129-38. https://doi.org/10.1159/000362695. 

3. Centre for Multilevel Modelling online course. 
http://www.bristol.ac.uk/cmm/learning/online-course/

Using data simulated by Rachel Hughes, this tutorial aims to replicate the 
analyses described in Hughes et al. cited above. It should be read in
conjuncture with this paper. However, not all of the methods used are currently
available in DataSHIELD. The key things it has not been possible to recreate are:

1. Multiple imputation
2. Spline models
3. 1-stage meta-analysis ("virtual pooling")

This tutorial is thus the best approximation of the methods outline in Hughes et 
al., using a two-stage rather than one-stage approach.

As and when these methods become available I will add to this tutorial.

Let's get started.

# Installing and loading packages

First we need to install the packages we need for the tutorial. If you already 
have these installed you can skip this section.

```{r install, eval = FALSE}
install.packages("remotes")
library(remotes)
install_github("datashield/DSI")
install_github("datashield/dsBaseClient", ref = "v6.2-dev")
install_github("lifecycle-project/ds-helper", ref = "maintenance")
install.packages("DSMolgenicArmadillo")
install.packages("MolgenisArmadillo")
install.packages("tidyverse")
install.packages("magrittr")
install.packages("kableExtra")
```

Now we load these packages.
```{r load-packages}
library(dsBaseClient)
library(DSI)
library(DSMolgenisArmadillo)
library(MolgenisArmadillo)
library(dsHelper)
library(tidyverse)
library(magrittr)
library(kableExtra)
```

# Logging in and assigning data

The simulated data is held on an Armadillo server. To access this, you first 
request a 'token' which contains the login details. You then use this to login 
and assign the data.
```{r login}
token <- armadillo.get_token("https://armadillo.test.molgenis.org")

builder <- DSI::newDSLoginBuilder()

builder$append(
server = "alspac",
url = "https://armadillo.test.molgenis.org",
table = "mlmalspac/tutorial/data",
token = token,
driver = "ArmadilloDriver")

builder$append(
server = "bcg",
url = "https://armadillo.test.molgenis.org",
table = "mlmbcg/tutorial/data",
token = token,
driver = "ArmadilloDriver")

builder$append(
server = "bib",
url = "https://armadillo.test.molgenis.org",
table = "mlmbib/tutorial/data",
token = token,
driver = "ArmadilloDriver")

builder$append(
server = "chs",
url = "https://armadillo.test.molgenis.org",
table = "mlmchs/tutorial/data",
token = token,
driver = "ArmadilloDriver")

builder$append(
server = "probit",
url = "https://armadillo.test.molgenis.org",
table = "mlmprobit/tutorial/data",
token = token,
driver = "ArmadilloDriver")

logindata <- builder$build()

conns <- DSI::datashield.login(logins = logindata, assign = TRUE, symbol = "data")
```
  
We can check that this has worked. There should be five cohorts, with a data 
frame for each cohort.
```{r summarise} 
ds.summary("data")
```
  
# Describing the data
Next we can produce descriptive statistics for our variables, recreating Table 1 
from Hughes et al. This takes quite a few steps, and requires some familiarity 
with the 'tidyverse' approach.  


## Extract statistics 
First we need to extract the relevant statistics. DataSHIELD contains the base 
functions ds.summary and ds.table to describe continuous and categorical 
variables. However, the output of these is tricky to work with. We can use the 
dh.getStats function from the dsHelper package to extract descriptives and put 
them in a more usable format.

```{r extract-stats}
stats <- dh.getStats(
	df = "data", 
	vars = c("id", "age", "weight", "sex", "pat_soc"),
	conns = conns)

mat_ed <- dh.getStats(
	df = "data", 
	vars = c("mat_ed"),
	conns = conns[c("alspac", "bib", "probit")])$categorical

eth <- dh.getStats(
	df = "data", 
	vars = "ethnicity",
	conns = conns[c("alspac", "bib")])$categorical

cat_stats <- bind_rows(stats$categorical, mat_ed, eth)
cont_stats <- stats$continuous
```
We now have two objects 'cat_stats' and 'cont_stats' containing descriptives for 
categorical and continuous variables. We will look at these later.

## Derive additional values

### Age range of participants
First we derive a variable indicating the age range of subjects within each 
cohort. Note, that for disclosure reasons it is not possible to view the minimum 
and maximum values. Instead we can view the range from 5% to 95%. 
```{r summarise-ages}
age_ranges <- cont_stats %>% 
filter(variable == "age" & cohort != "combined") %>%
mutate(range_95 = paste0(perc_5, " - ", perc_95)) %>%
select(variable, cohort, range_95) %>%
pivot_wider(names_from = "cohort", values_from = "range_95") %>%
mutate(category = "", variable = "Age range")
```

### Number of participants
We also need to calculate how many participants (not observations) there are 
within our data. This is a little convoluted in DataSHIELD. First, we use the 
function 'ds.tapply.assign' to summarise the number of observations for each 
subject. The length of this created object then gives us the number of subjects. 
```{r summarise-subjects}
ds.tapply.assign(
	X.name = "data$weight", 
	INDEX.names = "data$id", 
	FUN.name = "N", 
	newobj = "id_summary")

n_subjects <- ds.length("id_summary$N")[1:length(names(conns))] %>% 
setNames(names(conns)) %>% 
bind_rows() %>%
mutate(
	variable = "No. participants", 
	category = "") %>%
select(variable, everything()) %>%
mutate(across(where(is.numeric), as.character))
```

### Number of weight measurements per cohort
We can get this straightforwardly from the stats we extracted above.
```{r summarise-n-measurements}
weight_n <- cont_stats %>%
filter(variable == "weight" & cohort != "combined") %>%
select(cohort, variable, valid_n) %>%
pivot_wider(names_from = "cohort", values_from = "valid_n") %>%
mutate(category = "", variable = "Total no. weight measures") %>%
mutate(across(where(is.numeric), as.character))
```

### Median number of weight measurements per child
We can use the ds.quantileMean function with the object we created above (number
of measurements by id) to get the median number of measurements per child.
```{r summarise-median-measurements}
ds.asNumeric("id_summary$N", "id_summary_num")

weight_med_iqr <- ds.quantileMean("id_summary_num", type = "split") %>%
bind_rows(.id = "cohort") %>% 
select(cohort, "5%", "50%", "95%") %>%
rename(median = "50%", perc_5 = "5%", perc_95 = "95%") %>%
mutate(
	iqr = perc_95 - perc_5, 
	med_iqr = paste0(median, " (", iqr, ")"), 
	variable = "Median no. measures per child (IQR)", 
	category = "") %>%
select(variable, category, cohort, med_iqr) %>%
pivot_wider(names_from = cohort, values_from = med_iqr)
```

### Neaten up our extracted stats
We take the objects we created earlier with statistics on our exposures and 
covariates, select the summary information we want, reshape into wide format 
and relabel our categories.
```{r summarise-categorical}
cat_sum <- cat_stats %>%
select(variable, cohort, category, perc_total) %>% 
filter(cohort != "combined") %>%
pivot_wider(names_from = "cohort", values_from = "perc_total") %>% 
arrange(variable, category) %>%
mutate(category = case_when(
variable == "ethnicity" & category == 0 ~ "White",
variable == "ethnicity" & category == 1 ~ "South Asian",
variable == "ethnicity" & category == 2 ~ "Other",
variable == "ethnicity" & category == "missing" ~ "Missing",
variable == "sex" & category == 0 ~ "Male",
variable == "sex" & category == 1 ~ "Female",
variable == "sex" & category == "missing" ~ "Missing",
variable == "pat_soc" & category == 0 ~ "class I or II", 
variable == "pat_soc" & category == 1 ~ "class III", 
variable == "pat_soc" & category == 2 ~ "class IV, V or other",
variable == "pat_soc" & category == "missing" ~ "Missing",
variable == "mat_ed" & category == 0 ~ "Left school at 15 or 16",
variable == "mat_ed" & category == 1 ~ "Left school at 17 or 18",
variable == "mat_ed" & category == 2 ~ "Degree",
variable == "mat_ed" & category == "missing" ~ "Missing")) %>%
mutate(across(where(is.numeric), as.character)) %>%
mutate(variable = case_when(
	variable == "sex" ~ "Sex", 
	variable == "ethnicity" ~ "Ethnicity", 
	variable == "mat_ed" ~ "Maternal Education", 
	variable == "pat_soc" ~ "Paternal occupation")) 
```

### Median age at last measurement
We can also calculate the median age at last measurement for each cohort. 
It isn't super straightforward in DataSHIELD, as functions aren't designed to 
show min and max values. To get round this, we can use the function 
"dh.makeOutcome" to convert the repeated measures weight data to a single 
variable, selecting the latest observation with subjects have >1 observation.
```{r alm}
dh.makeOutcome(
	df = "data", 
	outcome = "weight", 
	age_var = "age", 
	bands = c(0, 18), 
	mult_action = "latest", 
	df_name = "latest_meas", 
	id_var = "id", 
	band_action = "g_le")

alm <- dh.getStats(
	df = "latest_meas", 
	vars = "age.18")$continuous 

alm_out <- alm %>%
select(cohort, perc_5, perc_50, perc_95) %>%
mutate(iqr = round(perc_95 - perc_50, 2),
	   val_iqr = paste0(perc_50, " (", iqr, ")")) %>%
select(cohort, val_iqr) %>%
pivot_wider(
	names_from = cohort, 
	values_from = val_iqr) %>%
mutate(
	variable = "Median ALMc in years (IQR)", 
	category = "")
```

### Table 1
Finally we can combine these, and use the 'kableExtra' package to make them into 
a nice html table.
```{r make-table, results = TRUE}
bind_rows(age_ranges, n_subjects, cat_sum, weight_n, weight_med_iqr, alm_out) %>%
rename(
	Variable = variable, 
	Category = category, 
	ALSPAC = alspac, 
	BCG = bcg, 
	BiB = bib, 
	CHS = chs,
	PROBIT = probit) %>%
select(Variable, Category, ALSPAC, BCG, BiB, CHS, PROBIT) %>%
kbl() %>% 
kable_styling(full_width = FALSE, position = "left") %>%
collapse_rows(valign = "top")
```


# Visualise outcome data
The next stage is to look at our observed weight data. 

## Scatter plots
We can use the scatterplot function to get an overview of the data. Weight 
increases with age, but it looks as though the relationship is non-linear.
```{r scatter}
ds.scatterPlot(x = "data$age", y = "data$weight", datasources = conns)
```

## Mean observed weight by age
We can also plot the mean weight by age for each cohort. It is a bit fiddly to 
do this in DataSHIELD, but it is possible. First we round age to the nearest 
year. We use a bit of a hack to do this as there isn't yet a 'round' function in 
datashield. 
```{r prep-weight-age}
ds.assign(
	toAssign = "data$age+0.5", 
	newobj = "age_tmp")

ds.asInteger("age_tmp", newobj = "age_round")
```
We then use the function meanSdGp to summarise weight at each age (1-18). We 
then put the data into long format so that we can plot it.
```{r mean-by-occasion}
cohort_ref <- tibble(
	cohort = c(names(conns), "combined"), 
	cohort_neat = c("ALSPAC", "BCG", "BiB", "CHS", "PROBIT", "Combined"))

observed <- names(conns) %>%
map(~ds.meanSdGp(x = "data$weight", y= "age_round", type = "split", datasources = conns[.]))

observed.plotdata <- observed %>%
map(function(x){

x$Mean_gp_stud %>%
as_tibble(rownames = "age") %>%
mutate(age = as.numeric(str_remove(age, "age_round_"))) %>%
pivot_longer(
	cols = -age,
	values_to = "number", 
	names_to = "cohort")
}) %>%
bind_rows() %>%
left_join(., cohort_ref, by = "cohort") %>%
select(-cohort) %>%
rename(Cohort = cohort_neat)
```
```{r plot}
palette <- c("#264653", "#2a9d8f", "#E9C46A", "#F4A261", "#E76F51")

obs_plot <- ggplot() + 
  geom_line(data = observed.plotdata, aes(x = age, y = number, colour = Cohort), size = 0.8) +
  scale_y_continuous(limit = c(0, 80), breaks = seq(0, 80, 20), expand = c(0, 0)) +
  xlab("Child age") +
  ylab("Observed weight (KG)") +
  scale_colour_manual(values = palette)
```

Here is the plot, which replicates Figure 1 in Rachel's paper.
```{r}
obs_plot
```  

Again, we can see that there is a non-linear relationship between height and age. 
We will fit fractional polynomial models to capture this non-linearity.

# Modelling strategy
As mention currently the 1-stage IPD approach described in Rachel's paper is 
not available. However in DataSHIELD we can do two-stage meta-analysis. 
I take the following approach here to approximate the approach described in 
Hughes et al. 

1. Take a random sample of the data within each cohort.
2. Fit a number of fractional polynomial models separately on each cohort using 
these sample.
3. Sum the negative log likelihood for each model across the cohorts.
4. Chose the model with the lowest log-likelihood across all cohorts.
5. Check the fit of this model on the remaining sample in each cohort.
6. Fit the best-fitting model on the full dataset including covariates.
7. Derive model-estimated values and standard errors.
8. Plot trajectories.
9. Compare observed and predicted values to describe model fit.

# Preparing the data for modelling

### Remove values of zero
Transforming values of zero will create infinite values which will break our 
models. We add a small quantity to our age variable to avoid this.

```{r add-small-value-to-age}
ds.assign(
	toAssign = "data$age+0.01", 
	newobj = "age")

dh.dropCols(
	df = "data", 
	vars = "age", 
	type = "remove", 
	comp_var = "id", 
	new_df_name = "data")

ds.cbind(
	x = c("data", "age"), 
	newobj = "data")
```

### Create transformations of age term
We can use the function ds.makeAgePolys to create transformations of age to the 
following powers: -2, -1, -0.5, log, 0.5, 2, 3. 
```{r make-polys}
dh.makeAgePolys(
	df = "data", 
	agevars = "age")
```

We can check this has worked, and see the mean values for these new variables.
```{r check-polys}
poly_summary <- dh.getStats(
	df = "data", 
	vars = c("agem_2", "agem_1", "agem_0_5", "agelog", "age0_5", "age2", "age3")) 
```

```{r show-polys, results = TRUE}
poly_summary$continuous %>%
select(cohort, variable, mean, std.dev) %>%
head(10) %>%
kbl() %>% 
kable_styling(full_width = FALSE, position = "left")
```

We also need to create an integer version of our ID variable. If we don't do 
this, then the lmer function breaks when we try to fit the models.
```{r integer-id}
ds.asInteger(
	x.name = "data$id", 
	newobj = "id_int")

ds.cbind(
	x = c("data", "id_int"),
	newobj = "data")
```

# Sampling the data
We create a random 30:70% split of the sample within each cohort. This currently
takes quite a few steps in DataSHIELD.

First we summarise the number of subjects per cohort.
```{r sample-first-step}
ref_sub <- n_subjects %>%
pivot_longer(
	cols = alspac:probit, 
	names_to = "cohort", 
	values_to = "n_subj") %>%
select(cohort, n_subj) %>%
mutate(n_subj = as.numeric(n_subj))
```

Next we create a new ID for each cohort, starting at 1 and increasing by 1
for each participant.
```{r new-id}
ref_sub %>%
pmap(function(cohort, n_subj){

ds.rep(
	x1 = c(1:n_subj), 
	times = "id_summary$N", 
	datasources = conns[cohort], 
	source.times = "serverside", 
	newobj = "new_id")

})

ds.cbind(x = c("data", "new_id"), newobj = "data_new_id")
```

Now create a vector indicating a random sample of 30% of individuals.
```{r sample-a-vector}
ref_sub %>%
pmap(function(cohort, n_subj){

ds.sample(
	x = n_subj,
	size = n_subj*0.3, 
	seed.as.integer = 456, 
	newobj = "new_id", 
	datasources = conns[cohort])
})
```

We create another vector indicating membership of the first sample and join 
it with the ID vector we created.
```{r sample-a-membership}
ds.assign(
	toAssign = "(new_id*0)+1",
	newobj = "sample")

ds.cbind(
	x = c("new_id", "sample"),  
	newobj = "sample_a_tmp")
```

We merge this dataframe back with the original dataframe. In the vector 
indicating which sample the subject belongs to, we replace missing values with 
2. This is because everyone not in the original sample will have a value of NA.
```{r sample-b-membership}
ds.merge(
	x.name = "data_new_id", 
	y.name = "sample_a_tmp", 
	by.x.names = "new_id",
	by.y.names = "new_id",
	newobj = "sample_merged", 
	all.x = TRUE)

na_replace_vec <- rep("2", length(conns))

ds.replaceNA(
	x = "sample_merged$sample", 
	forNA = na_replace_vec, 
	newobj = "sample")

ds.cbind(
	x = c("data", "sample"), 
	newobj = "data")
```

Now we create two subsets: sample A (30%) and sample B (70%).
```{r subset-samples}
ds.dataFrameSubset(
	df.name = "data", 
	V1.name = "data$sample", 
	V2.name = "1", 
	Boolean.operator = "==", 
	keep.NAs = FALSE, 
	newobj = "sample_a")

ds.dataFrameSubset(
	df.name = "data", 
	V1.name = "data$sample", 
	V2.name = "2", 
	Boolean.operator = "==", 
	keep.NAs = FALSE, 
	newobj = "sample_b")
```


# Identifying the best fitting model using the discovery sample
The DataSHIELD function for mixed effects is ds.lmerSLMA. To streamline the fitting of multiple models, we can also use
the functions "dh.makeLmerForm" and "dh.lmeMultPoly" from the dsHelper package.

## Create model formulae
First we create a table containing formulae for all of the models that we
want to compare. The argument to "agevars" is a vector of variable names corresponding
to the age transformations we created above. 
```{r make-formula, results = TRUE}
weight_form <- dh.makeLmerForm(
  outcome = "weight", 
  idvar = "id_int", 
  agevars = c("age", "agem_2", "agem_1", "agem_0_5", 
              "agelog", "age0_5", "age2", "age3"))

weight_form %>% 
head(10) %>%
kbl() %>% 
kable_styling(full_width = FALSE, position = "left") 
```

This has created a table where each row (total of 28) contains the 
formula for different combinations of the fractional polynomials. At the moment 
this produces formulae to fit random intercept models. I am struggling to
get random slope models to converge and will discuss this with Paul Burton.

## Fit all combinations of polynomials
The function dh.lmeMultPoly takes as an input the table we have just created.
It then automatically fits all of these models.
```{r sample-a-fit}
sample_a.fit <- dh.lmeMultPoly(
	df = "sample_a",
	formulae = weight_form)
```

The output contains a table with the negative log likelihood for each model in
each study, the average rank of that model across all studies and
the summed negative log likelihood. The model with the lowest summed 
log-likelhood across studies contains two powers: age^0.5 & age^2.

```{r show-sample-a-fit, results = TRUE}
sample_a.fit$fit %>%
select(model, loglik_study1:loglik_study5, sum_log) %>%
arrange(desc(sum_log)) %>%
head(10) %>%
kbl() %>% 
kable_styling(full_width = FALSE, position = "left")
```

# Testing fit of model 
Next, we can fit the same models to the validation dataset and compare fit.
```{r sample-b-fit}
sample_b.fit <- dh.lmeMultPoly(
	df = "sample_b",
	formulae = weight_form) 
```

As our data was simulated, we have an unrealistic situation where the models fit 
exactly as well in each cohort. However, this demonstrates the principle. 
```{r show-sample-b-fit, results = TRUE}
sample_b.fit$fit %>%
select(model, loglik_study1:loglik_study5, sum_log) %>%
arrange(desc(sum_log)) %>%
head(10) %>%
kbl() %>% 
kable_styling(full_width = FALSE, position = "left")
```

# Fitting final model including covariates
Unfortunately, multiple imputation is not yet available within DataSHIELD. In 
this tutorial we fit a restricted version of the model specified by Rachel, 
excluding covariates with large degrees of missingness. Given this, the only 
covariate we can include is paternal occupation. We fit this model using the 
full sample.

```{r final-model}
sample_full.fit <- ds.lmerSLMA(
	dataName = "data",
    formula = "weight ~ 1 + age0_5 + age2 + pat_soc + pat_soc*age0_5 + pat_soc*age2 + (1|id_int)")
```

# Predicted values and standard errors
We can now derive model estimated values for height for different values of age, 
and plot these to visualise the modelled trajecotories. Currently there is no 
predict method for lmer within datashield, so we have to do this manually. 

## Predicted values
This takes a few steps at present. First, we create a table holding the age 
ranges for each cohort.
```{r ages}
ages_ref <- dh.getStats(
  df = "data",
  vars = "age"
)
```

Now we extract coefficients from the model using the dsHelper function 
"dh.lmTab". We also rename the interaction coefficients to have more useable 
names.
```{r coefs}
coefs <- dh.lmTab(
  model = sample_full.fit, 
  type = "lmer",
  coh_names = names(conns),
  direction = "long", 
  ci_format = "separate") %>%
  filter(coefficient == "est") %>%
  pivot_wider(
    names_from = variable,
    values_from = value
  ) %>%
  dplyr::rename(
  	age_0_5_pat_soc_1 = "age0_5:pat_soc1", 
  	age_0_5_pat_soc_2 = "age0_5:pat_soc2", 
    age_2_pat_soc_1 = "age2:pat_soc1", 
  	age_2_pat_soc_2 = "age2:pat_soc2")
```

Next we derive predicted values at every 0.01 month, for each cohort, based on 
the age range in the observed data for that cohort. We also get predicted values 
at each combination of levels for paternal occupation.
```{r predicted}
pred <- coefs %>%
  pmap(
    function(cohort, intercept, age0_5, age2, pat_soc1, pat_soc2, age_0_5_pat_soc_1, age_0_5_pat_soc_2, age_2_pat_soc_1, age_2_pat_soc_2, ...) {
      
      pred_pat_0 <- tibble(
        age = seq(0, 18, by = 0.01),
        age_0_5_ref = age^0.5,
        age_2_ref = age^2,
        cohort = cohort,
        pat_1_ref = 0, 
        pat_2_ref = 0,
        predicted = intercept + age_0_5_ref*age0_5 + age_2_ref*age2 + pat_1_ref*pat_soc1 + pat_2_ref*pat_soc2 + 
        age_0_5_ref*pat_1_ref*age_0_5_pat_soc_1 + age_0_5_ref*pat_2_ref*age_0_5_pat_soc_2 + 
        age_2_ref*pat_1_ref*age_2_pat_soc_1 + age_2_ref*pat_2_ref*age_2_pat_soc_2
      )

      pred_pat_1 <- tibble(
        age = seq(0, 18, by = 0.01),
        age_0_5_ref = age^0.5,
        age_2_ref = age^2,
        cohort = cohort,
        pat_1_ref = 1, 
        pat_2_ref = 0,
        predicted = intercept + age_0_5_ref*age0_5 + age_2_ref*age2 + pat_1_ref*pat_soc1 + pat_2_ref*pat_soc2 + 
        age_0_5_ref*pat_1_ref*age_0_5_pat_soc_1 + age_0_5_ref*pat_2_ref*age_0_5_pat_soc_2 + 
        age_2_ref*pat_1_ref*age_2_pat_soc_1 + age_2_ref*pat_2_ref*age_2_pat_soc_2
      )

      pred_pat_2 <- tibble(
        age = seq(0, 18, by = 0.01),
        age_0_5_ref = age^0.5,
        age_2_ref = age^2,
        cohort = cohort,
        pat_1_ref = 0, 
        pat_2_ref = 1,
        predicted = intercept + age_0_5_ref*age0_5 + age_2_ref*age2 + pat_1_ref*pat_soc1 + pat_2_ref*pat_soc2 + 
        age_0_5_ref*pat_1_ref*age_0_5_pat_soc_1 + age_0_5_ref*pat_2_ref*age_0_5_pat_soc_2 + 
        age_2_ref*pat_1_ref*age_2_pat_soc_1 + age_2_ref*pat_2_ref*age_2_pat_soc_2
      )

      pred <- bind_rows(pred_pat_0, pred_pat_1, pred_pat_2)
      
      return(pred)
    }
  ) %>% bind_rows() 
```

Now we trim these estimates to include only values present in the observed data.
```{r trim}
age_min_max <- ages_ref$continuous %>% 
  select(cohort_ref = cohort, perc_5, perc_95) %>%
  mutate(perc_95 = ifelse(cohort_ref == "combined", 17.9, perc_95))

pred <- age_min_max %>%
  pmap(function(cohort_ref, perc_5, perc_95, ...){
    
pred %>%
      filter(cohort == cohort_ref & between(age, perc_5, perc_95))
  }) %>% bind_rows()
```

## Standard errors
Next we need to calculate standard errors for the predicted values. Currently we 
do this from the variance-covariance matrices. Again, once a predict function is 
developed for lmer all of this should be easier. We have not yet been able to 
calculate the standard errors for the combined estimates.
```{r ses}
pred_coh <- pred %>% 
  filter(cohort != "combined") %>%
  mutate(study_ref = case_when(
    cohort == "alspac" ~ "study1",
    cohort == "bcg" ~ "study2",
    cohort == "bib" ~ "study3",
    cohort == "chs" ~ "study4",
    cohort == "probit" ~ "study5"))

pred_coh_se <- pred_coh %>%
  pmap(function(study_ref, age_0_5_ref, age_2_ref, pat_1_ref, pat_2_ref, ...){
    
    vcov <- sample_full.fit$output.summary[[study_ref]]$vcov
    C <- c(1, age_0_5_ref, age_2_ref, pat_1_ref, pat_2_ref, age_0_5_ref*pat_1_ref, age_0_5_ref*pat_2_ref, age_2_ref*pat_1_ref, age_0_5_ref*pat_2_ref)
    std.er <- sqrt(t(C) %*% vcov %*% C)
    out <- std.er@x
    return(out)}) %>%
  unlist

pred_coh %<>% 
  mutate(se = pred_coh_se) %>%
  mutate(low_ci = predicted - 1.96*se, 
         upper_ci = predicted + 1.96*se)

comb.pred <- pred %>%
  filter(cohort == 'combined') %>%
  mutate(low_ci = predicted, upper_ci = predicted)

coh_labels <- c("ALSPAC", "BCG", "BiB", "CHS", "PROBIT", "Combined")

pred.plotdata <- bind_rows(pred_coh, comb.pred) %>%
left_join(., cohort_ref, by = "cohort") %>%
select(-cohort) %>%
rename(cohort = cohort_neat) %>%
mutate(
	cohort = factor(cohort, labels = coh_labels, levels = coh_labels, ordered = TRUE))
```

We can see the table of values we have made
```{r, show-predicted, results = TRUE}
pred.plotdata %>%
  select(age, cohort, predicted, se, low_ci, upper_ci) %>%
  head(10) %>%
kbl() %>% 
kable_styling(full_width = FALSE, position = "left")
```

# Visualise trajectories
Now we can plot these estimated values. Although we've calculated confidence 
intervals I haven't included them in the plots so that the trajectories are 
clearer.

## Predicted trajectory by cohort
```{r traj-plots}
cohort.plotdata <- pred.plotdata %>% filter(pat_1_ref == 0 & pat_2_ref == 0 & cohort != "Combined")

ggplot() + 
  geom_line(data = cohort.plotdata, aes(x = age, y = predicted, colour = cohort), size = 0.4) +
  scale_colour_manual(values = palette) + 
  scale_x_continuous(limit = c(0, 20), breaks = seq(0, 20, 5), expand = c(0, 0)) + 
  scale_y_continuous(limit = c(0, 80), breaks = seq(0, 80, 20), expand = c(0, 0)) +
  xlab("Child age") +
  ylab("Predicted weight") +
  labs(colour = "Cohort")
```

## Predicted trajectory by paternal occupation
```{r plot-pat-ed}
pat.plotdata <- pred.plotdata %>% 
filter(cohort == "Combined") %>%
mutate(pat_occ = case_when(
pat_1_ref == 0 & pat_2_ref == 0 ~ "class I or II",
pat_1_ref == 1 & pat_2_ref == 0 ~ "class III",
pat_1_ref == 0 & pat_2_ref == 1 ~ "class IV, V or other")) %>%
mutate(pat_occ = factor(pat_occ, ordered = TRUE))

ggplot() + 
  geom_line(data = pat.plotdata, aes(x = age, y = predicted, colour = pat_occ), size = 0.4) +
  scale_colour_manual(values = palette) + 
  scale_x_continuous(limit = c(0, 20), breaks = seq(0, 20, 5), expand = c(0, 0)) + 
  scale_y_continuous(limit = c(0, 80), breaks = seq(0, 80, 20), expand = c(0, 0)) +
  xlab("Child age") +
  ylab("Predicted weight") +
  labs(colour = "Paternal occupation") 
```


# Checking model fit
The final step is to see how well the model fits at different age points. Here 
we can approximate Table 2 from Tilling et al. (2014) "Modelling Childhood 
Growth". 

First we need to get mean observed height values between key ages points. There 
are quite a few steps required to do this in DataSHIELD. First we need to create 
a table defining our age bands.
```{r, residuals-a}
bands <- c(0, 1, 1, 2, 3, 5, 6, 10, 11, 15, 16, 18)

pairs <- split(bands, ceiling(seq_along(bands) / 2))

subnames <- unlist(
  pairs %>% map(~ paste0("res", "_", paste0(., collapse = "_"))),
  use.names = FALSE
)

cats <- tibble(
    varname = rep(subnames, each = 2),
    value = bands,
    op = rep(c(">=", "<="), times = (length(bands) / 2)),
    tmp = ifelse(op == ">=", "gte", "lte"),
    new_df_name = paste0("res", tmp, value)
  )
```

Now we create vectors of 1s and 0s indicating whether the above criteria are
met for each subject.
```{r residuals-b}
cats %>%
  pmap(function(value, op, new_df_name, ...) {
    ds.Boole(
      V1 = "data$age",
      V2 = value,
      Boolean.operator = op,
      newobj = new_df_name
    )
  })

suppressMessages(
  assign_conditions <- cats %>%
    group_by(varname) %>%
    summarise(condition = paste(new_df_name, collapse = "*"))
)

assign_conditions %>%
  pmap(function(condition, varname) {
    ds.assign(
      toAssign = condition,
      newobj = varname
    )
  })
```

We then use these vectors to summarise mean observed height at the age periods 
we are interested in.
```{r residuals-c}
obs_by_agecat_comb <- assign_conditions %>%
pull(varname) %>%
map(
~ ds.meanSdGp(
	x = "data$weight", 
	y = ., 
	type = "split")
	)
```

Now we take these values and put them into a neater table
```{r residuals-d}
res_obs <- obs_by_agecat_comb %>%
map(function(x){

x$Mean_gp %>%
as_tibble(rownames = "age") %>%
slice(2)
}) %>%
bind_rows() %>%
mutate(
	age_cat = case_when(
		age == "res_0_1_2" ~ "0_1", 
		age == "res_1_2_2" ~ "1_2", 
		age == "res_11_15_2" ~ "11_15", 
		age == "res_3_5_2" ~ "3_5", 
		age == "res_6_10_2" ~ "6_10", 
	    age == "res_16_18_2" ~ "16_18")) %>%
select(-age) %>%
pivot_longer(
	cols = alspac:probit, 
	values_to = "observed",
	names_to = "cohort") %>%
left_join(., cohort_ref, by = "cohort") %>%
select(-cohort) %>%
rename(cohort = cohort_neat) %>%
mutate(observed = round(observed, 2))
```

Next we take our predicted values, put them into the same format and merge with
our observed values.
```{r residuals-e}
res_pred <- pred.plotdata %>%
mutate(
	age_cat = case_when(
		age > 0 & age <= 1 ~ "0_1", 
		age >= 1 & age <= 2 ~ "1_2", 
		age >= 3 & age <= 5 ~ "3_5", 
		age >= 6 & age <= 10 ~ "6_10", 
		age >= 11 & age <= 15 ~ "11_15", 
		age >= 16 & age <= 18 ~ "16_18")
		) %>%
filter(!is.na(age_cat)) %>%
group_by(age_cat, cohort) %>%
summarise(
	predicted = round(mean(predicted), 2),
	low_ci = round(mean(low_ci), 2),
	upper_ci = round(mean(upper_ci), 2))

res_tab <- left_join(res_obs, res_pred, by = c("age_cat", "cohort")) %>%
mutate(
	difference = round(observed - predicted, 2),
	lower_res = round(observed - upper_ci, 2),
	higher_res = round(observed - low_ci, 2),
	age_cat = factor(
		age_cat, 
		levels = c("0_1", "1_2", "3_5", "6_10", "11_15", "16_18"),
		ordered = TRUE), 
	limits = paste0(lower_res, " to ", higher_res)) %>%
filter(!is.na(predicted)) %>%
select(age_cat, cohort, observed, predicted, difference, limits) %>%
arrange(age_cat) %>%
mutate(age_cat = case_when(
	age_cat == "0_1" ~ "Birth to <= 1", 
	age_cat == "1_2" ~ "> 1 to <= 2",
	age_cat == "3_5" ~ ">= 3 to <= 5",
	age_cat == "6_10" ~ ">= 6 to <= 10",
	age_cat == "11_15" ~ ">= 11 to <= 15", 
	age_cat == "16_18" ~ ">= 16 to <= 18")) %>%
rename(
	"Growth period" = age_cat, 
	"Mean observed height, cm (SD)" = observed,
	"Mean predicted height" = predicted,
	"Mean difference" = difference,
	"95% limits of agreement" = limits, 
	"Cohort" = cohort)
```

Finally we produce our table showing observed vs predicted values at different
age points. You can see that model fit isn't great for PROBIT towards the 
extremes.
```{r model-check, results = TRUE}
res_tab %>%
kbl(full_width = FALSE) %>% 
kable_styling(full_width = FALSE, position = "left") %>%
collapse_rows(valign = "top")
```

# Things still to address

1. Modelling the covariance structure
2. Fitting random slope models

